{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54523d32-290a-4176-9137-d57013ef8aaf",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "## Mathematical Understanding\n",
    "Logistic regression is a way to map continuous data to binary outputs. What it essentially means is that by using such a model we input a bunch of parameters and seek to know the output of a binary question i.e. whether the output would be a Yes(1) or a No(0).\n",
    " \n",
    "So an excellent example would be:\n",
    "> Given the following study habits (number of hours studied, number of books read), what is the probability that the student passes i.e. how do the studying habits affect the chances of getting a pass in the exam. \n",
    "\n",
    "Here, by taking the exact example, we are mapping a function \n",
    "$$\n",
    "f: (n_s, n_b) \\rightarrow \\{0,1\\}\n",
    "$$\n",
    "\n",
    "However, we do not model it as a function spitting a binary output, but rather as giving the probability-giving function:\n",
    "\n",
    "$$\n",
    "p: (n_s, n_b) \\rightarrow (0,1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bda2841-89a9-4eb1-9276-9d916f64affb",
   "metadata": {},
   "source": [
    "### Model\n",
    "Taking the simple case of only taking one input, $p$ becomes a one-dimensional function, whose ansatz is:\n",
    "$$\n",
    "p(x)=\\frac{1}{1+e^{-(\\beta_0+\\beta_1 x)}}\n",
    "$$\n",
    "We can check that this function only outputs within the range $(0,1]$\n",
    "Our whole task in this regression model then becomes to find out $\\beta_0$ and $\\beta_1$ that best fit the input data. $p(x)$ is now the probability that at $x$ the output is 1, and conversely $1-p(x)$ is the probability that the output is 0.\n",
    "\n",
    "To generalise for $n$-dimensional input $\\bold{x}$:\n",
    "$$\n",
    "p(\\bold{x})=\\frac{1}{1+e^{-(\\boldsymbol{\\beta} \\cdot \\bold{x}+\\beta_0)}}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\beta}$ is a vector that is dotted with the vector $\\bold{x}$ (by vector here I just mean, an array of numbers so that I could compactify the  long multiplication as a simple dot product). In this case, we now have to determine each $n$ components of $\\boldsymbol{\\beta}$ and $\\beta_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d88857-9ec2-46bf-8d3e-50ff332f1f00",
   "metadata": {},
   "source": [
    "### Fit\n",
    "Now, to find out the above function, we need to fit the curve to the data that we already have. \n",
    "\n",
    "In most of the cases, we do this by finding a suitable loss function, which tracks how much our model deviated from the output that we obtained in the real world. In this case we can use log-loss as a function to calculate the loss of a specific data point and then sum them up.\n",
    "\n",
    "To calculate the log-loss, we go about as follows:\n",
    "$$\n",
    "Logloss(x_k)=\\begin{cases}\n",
    "-ln(p_k) \\ if \\ y_k=1\\\\\n",
    "-ln(1-p_k) \\ if \\ y_k=0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This can more neatly be written as $-(y_k(ln(p_k))+(1-y_k)(ln(1-p_k)))$\n",
    "\n",
    "To copy some fancy jargon(formal language) from Wikipedia:\n",
    "This expression is more formally known as the cross entropy of the predicted distribution $(p_{k},(1-p_{k}))$ from the actual distribution $(y_{k},(1-y_{k}))$, as probability distributions on the two-element space of (pass, fail)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501e8fd1-0970-458f-8895-1041fcc0f649",
   "metadata": {},
   "source": [
    "Thus, the actual stuff to be minimised is:\n",
    "$$\n",
    "l=\\frac{1}{n}\\sum_{k=1}^n -((y_k(ln(p_k))+(1-y_k)(ln(1-p_k))))\n",
    "$$\n",
    "\n",
    "Or, if we just remove the - sign from the RHS, we reduce the problem to maximising $-l$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e971a2-5cbd-4301-8567-9492ec54d797",
   "metadata": {},
   "source": [
    "#### Log odds and assumptions\n",
    "Lets define odds as follows(we run with the 1-d data case here): \n",
    "$$\n",
    "o(x)=\\frac{p(x)}{1-p(x)}=e^{\\beta_0+\\beta_1 x}\n",
    "$$\n",
    "\n",
    "Log-odds thus is:\n",
    "$$\n",
    "\\log(o(x))=\\beta_0+\\beta_1 x\n",
    "$$\n",
    "\n",
    "The basic assumption of logistic regression is that the log odds of probability is linearly dependent on the input variables, which may not necessarily be true."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d2cfdd-dcf4-41e9-bdb1-f12ff6f3bb27",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Estimating the parameters $\\boldsymbol{\\beta}$ and $\\beta_0$\n",
    "\n",
    "This now boils down to simple multivariable calculus, where to find out the extrema of a function is to take the partial derivative with respect to each of the input variables and setting them to $0$:\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial \\beta_i}= 0 \n",
    "$$\n",
    "for $i \\in \\{0,1,...,n\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb48f17-f1ac-4ebe-94f9-e59824224eab",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "We will use gradient descent. First let calculate the explicit formulas for the gradients we take for n-dimensional input values \n",
    "\n",
    "For, $\\beta_m$ $m\\in \\{0,1,..., n\\}$, with $N$ data points and where $x_{0.i}= 1$:\n",
    "$$\n",
    "    \\frac{\\partial l}{\\partial \\beta_m}= \\frac{1}{N}\\sum_{i=1}^N y_i(x_{m,i} (1-p_i))+ (1-y_i)(x_{m,i} p_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{N}\\sum_{i=1}^N x_{m,i}(y_i-p_i)\n",
    "$$\n",
    "\n",
    "Thus the gradient:\n",
    "$$\n",
    "\\nabla l=\\begin{bmatrix}\n",
    "     \\frac{\\partial l}{\\partial \\beta_0}\\\\\n",
    "     \\frac{\\partial l}{\\partial \\beta_1}\\\\\n",
    "     \\vdots\\\\\n",
    "     \\frac{\\partial l}{\\partial \\beta_n}\\\\\n",
    "    \\end{bmatrix}=\n",
    "    \\begin{bmatrix}\n",
    "    \\frac{1}{N}\\sum_{i=0}^{N} (y_i-p_i)x_{0,i}\\\\\n",
    "    \\frac{1}{N}\\sum_{i=0}^{N} (y_i-p_i)x_{1,i}\\\\\n",
    "    \\vdots\\\\\n",
    "    \\frac{1}{N}\\sum_{i=0}^{N} (y_i-p_i)x_{n,i}\\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6c3c5c-1fbd-4054-817e-1aea44a455fd",
   "metadata": {},
   "source": [
    "## Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d89c6e7-2934-41b1-b966-87e238f61401",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definitions\n",
    "import numpy as np\n",
    "from numpy import log,dot,e,shape\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58568a90-db52-4809-b802-8baf3887eaf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return (1+e**(-x))**(-1)\n",
    "\n",
    "\n",
    "class Logistic_Regression():\n",
    "    def __init__(self, lr=0.01, n_iters=1000000):\n",
    "        self.lr=lr\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            linear_pred = np.dot(X, self.weights) + self.bias\n",
    "            predictions = sigmoid(linear_pred)\n",
    "\n",
    "            dw = (1/n_samples) * np.dot(X.T, (predictions - y))\n",
    "            db = (1/n_samples) * np.sum(predictions-y)\n",
    "\n",
    "            self.weights = self.weights - self.lr*dw\n",
    "            self.bias = self.bias - self.lr*db\n",
    "            \n",
    "           \n",
    "    def predict(self, X):\n",
    "        linear_pred = np.dot(X, self.weights)+ self.bias\n",
    "        y_pred = sigmoid(linear_pred)\n",
    "        class_pred=[]\n",
    "        for i in y_pred:\n",
    "            if i>=0.5:\n",
    "                class_pred.append(1)\n",
    "            else:\n",
    "                class_pred.append(0)\n",
    "        return class_pred\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec8e3e42-2b05-4139-af38-890c5ce742a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = Logistic_Regression()\n",
    "#Training data\n",
    "df=pd.read_csv('data/ds1_train.csv')\n",
    "X_0=df.drop(columns=[\"y\"])\n",
    "y_0=df[\"y\"]\n",
    "X0=X_0.to_numpy()\n",
    "y0=y_0.to_numpy().flatten()\n",
    "\n",
    "#Testing data\n",
    "df1=pd.read_csv('data/ds1_test.csv')\n",
    "X_0=df1.drop(columns=[\"y\"])\n",
    "y_0=df1[\"y\"]\n",
    "X1=X_0.to_numpy()\n",
    "y1=y_0.to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48d9f010-e8a2-43dc-b915-5e63042b9714",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf.fit(X0, y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b9bbd99-b172-4e0b-9cf7-6999b9277753",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.82\n",
      "Confusion Matrix: \n",
      " [[34  2]\n",
      " [16 48]]\n"
     ]
    }
   ],
   "source": [
    "y_pred=clf.predict(X1)\n",
    "\n",
    "def accuracy(y_pred, y_test):\n",
    "    return np.sum(y_pred==y_test)/len(y_test)\n",
    "\n",
    "acc = accuracy(y_pred, y1)\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(y_pred, y1))\n",
    "#An unusuaal number of false positives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa9e20b-60b2-4b08-a77c-de4d253984f2",
   "metadata": {},
   "source": [
    "## Scikit-learn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ee09f25-cdd3-4565-9516-e70647d682a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_0,y_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1645bc8a-93e8-4c51-9b7c-2523f088cdef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9\n",
      "Confusion Matrix: \n",
      " [[43  3]\n",
      " [ 7 47]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssheersh/miniconda3/lib/python3.10/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_pred_scikit=model.predict(X1)\n",
    "acc = accuracy(y_pred_scikit, y1)\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(y_pred_scikit, y1))\n",
    "#An unusuaal number of false positives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39284847-6d13-4475-bf61-f51c869b234e",
   "metadata": {},
   "source": [
    "#### **Vanilla implementation's accuracy**: $82\\%$\n",
    "\n",
    "#### **Scikit-Learn pre-built's accuracy**: $90\\%$\n",
    "\n",
    "So the Scikit-Learn is fairly more accurate,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
