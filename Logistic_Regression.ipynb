{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54523d32-290a-4176-9137-d57013ef8aaf",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "## Mathematical Understanding\n",
    "Logistic regression is a way to map continuous data to binary outputs. What it essentially means is that by using such a model we input a bunch of parameters and seek to know the output of a binary question i.e. whether the output would be a Yes(1) or a No(0).\n",
    " \n",
    "So an excellent example would be:\n",
    "> Given the following study habits (number of hours studied, number of books read), what is the probability that the student passes i.e. how do the studying habits affect the chances of getting a pass in the exam. \n",
    "\n",
    "Here, by taking the exact example, we are mapping a function \n",
    "$$\n",
    "f: (n_s, n_b) \\rightarrow \\{0,1\\}\n",
    "$$\n",
    "\n",
    "However, we do not model it as a function spitting a binary output, but rather as giving the probability-giving function:\n",
    "\n",
    "$$\n",
    "p: (n_s, n_b) \\rightarrow (0,1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bda2841-89a9-4eb1-9276-9d916f64affb",
   "metadata": {},
   "source": [
    "### Model\n",
    "Taking the simple case of only taking one input, $p$ becomes a one-dimensional function, whose ansatz is:\n",
    "$$\n",
    "p(x)=\\frac{1}{1+e^{-(\\beta_0+\\beta_1 x)}}\n",
    "$$\n",
    "We can check that this function only outputs within the range $(0,1]$\n",
    "Our whole task in this regression model then becomes to find out $\\beta_0$ and $\\beta_1$ that best fit the input data. $p(x)$ is now the probability that at $x$ the output is 1, and conversely $1-p(x)$ is the probability that the output is 0.\n",
    "\n",
    "To generalise for $n$-dimensional input $\\bold{x}$:\n",
    "$$\n",
    "p(\\bold{x})=\\frac{1}{1+e^{-(\\boldsymbol{\\beta} \\cdot \\bold{x}+\\beta_0)}}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\beta}$ is a vector that is dotted with the vector $\\bold{x}$ (by vector here I just mean, an array of numbers so that I could compactify the  long multiplication as a simple dot product). In this case, we now have to determine each $n$ components of $\\boldsymbol{\\beta}$ and $\\beta_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d88857-9ec2-46bf-8d3e-50ff332f1f00",
   "metadata": {},
   "source": [
    "### Fit\n",
    "Now, to find out the above function, we need to fit the curve to the data that we already have. \n",
    "\n",
    "In most of the cases, we do this by finding a suitable loss function, which tracks how much our model deviated from the output that we obtained in the real world. In this case we can use log-loss as a function to calculate the loss of a specific data point and then sum them up.\n",
    "\n",
    "To calculate the log-loss, we go about as follows:\n",
    "$$\n",
    "Logloss(x_k)=\\begin{cases}\n",
    "-ln(p_k) \\ if \\ y_k=1\\\\\n",
    "-ln(1-p_k) \\ if \\ y_k=0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This can more neatly be written as $-(y_k(ln(p_k))+(1-y_k)(ln(1-p_k)))$\n",
    "\n",
    "To copy some fancy jargon(formal language) from Wikipedia:\n",
    "This expression is more formally known as the cross entropy of the predicted distribution $(p_{k},(1-p_{k}))$ from the actual distribution $(y_{k},(1-y_{k}))$, as probability distributions on the two-element space of (pass, fail)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501e8fd1-0970-458f-8895-1041fcc0f649",
   "metadata": {},
   "source": [
    "Thus, the actual stuff to be minimised is:\n",
    "$$\n",
    "l=\\frac{1}{n}\\sum_{k=1}^n -((y_k(ln(p_k))+(1-y_k)(ln(1-p_k))))\n",
    "$$\n",
    "\n",
    "Or, if we just remove the - sign from the RHS, we reduce the problem to maximising $-l$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e971a2-5cbd-4301-8567-9492ec54d797",
   "metadata": {},
   "source": [
    "#### Log odds and assumptions\n",
    "Lets define odds as follows(we run with the 1-d data case here): \n",
    "$$\n",
    "o(x)=\\frac{p(x)}{1-p(x)}=e^{\\beta_0+\\beta_1 x}\n",
    "$$\n",
    "\n",
    "Log-odds thus is:\n",
    "$$\n",
    "\\log(o(x))=\\beta_0+\\beta_1 x\n",
    "$$\n",
    "\n",
    "The basic assumption of logistic regression is that the log odds of probability is linearly dependent on the input variables, which may not necessarily be true."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d2cfdd-dcf4-41e9-bdb1-f12ff6f3bb27",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Estimating the parameters $\\boldsymbol{\\beta}$ and $\\beta_0$\n",
    "\n",
    "This now boils down to simple multivariable calculus, where to find out the extrema of a function is to take the partial derivative with respect to each of the input variables and setting them to $0$:\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial \\beta_i}= 0 \n",
    "$$\n",
    "for $i \\in \\{0,1,...,n\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb48f17-f1ac-4ebe-94f9-e59824224eab",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "We will use gradient descent. First let calculate the explicit formulas for the gradients we take for n-dimensional input values \n",
    "\n",
    "For, $\\beta_m$ $m\\in \\{0,1,..., n\\}$, with $N$ data points and where $x_{0.i}= 1$:\n",
    "$$\n",
    "    \\frac{\\partial l}{\\partial \\beta_m}= \\frac{1}{N}\\sum_{i=1}^N y_i(x_{m,i} (1-p_i))+ (1-y_i)(x_{m,i} p_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{N}\\sum_{i=1}^N x_{m,i}(y_i-p_i)\n",
    "$$\n",
    "\n",
    "Thus the gradient:\n",
    "$$\n",
    "\\nabla l=\\begin{bmatrix}\n",
    "     \\frac{\\partial l}{\\partial \\beta_0}\\\\\n",
    "     \\frac{\\partial l}{\\partial \\beta_1}\\\\\n",
    "     \\vdots\\\\\n",
    "     \\frac{\\partial l}{\\partial \\beta_n}\\\\\n",
    "    \\end{bmatrix}=\n",
    "    \\begin{bmatrix}\n",
    "    \\frac{1}{N}\\sum_{i=0}^{N} (y_i-p_i)x_{0,i}\\\\\n",
    "    \\frac{1}{N}\\sum_{i=0}^{N} (y_i-p_i)x_{1,i}\\\\\n",
    "    \\vdots\\\\\n",
    "    \\frac{1}{N}\\sum_{i=0}^{N} (y_i-p_i)x_{n,i}\\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6c3c5c-1fbd-4054-817e-1aea44a455fd",
   "metadata": {},
   "source": [
    "## Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d89c6e7-2934-41b1-b966-87e238f61401",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.911809</td>\n",
       "      <td>60.359613</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.774746</td>\n",
       "      <td>344.149284</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.615488</td>\n",
       "      <td>178.222087</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.013694</td>\n",
       "      <td>15.259472</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.757625</td>\n",
       "      <td>66.194174</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        x_1         x_2    y\n",
       "0  2.911809   60.359613  0.0\n",
       "1  3.774746  344.149284  0.0\n",
       "2  2.615488  178.222087  0.0\n",
       "3  2.013694   15.259472  0.0\n",
       "4  2.757625   66.194174  0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definitions\n",
    "import numpy as np\n",
    "from numpy import log,dot,e,shape\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('data/ds1_train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "58568a90-db52-4809-b802-8baf3887eaf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return (1+e**(-x))**(-1)\n",
    "\n",
    "\n",
    "class LogisticRegression():\n",
    "    def __init__(self, lr=0.01, n_iters=1000000):\n",
    "        self.lr=lr\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            linear_pred = np.dot(X, self.weights) + self.bias\n",
    "            predictions = sigmoid(linear_pred)\n",
    "\n",
    "            dw = (1/n_samples) * np.dot(X.T, (predictions - y))\n",
    "            db = (1/n_samples) * np.sum(predictions-y)\n",
    "\n",
    "            self.weights = self.weights - self.lr*dw\n",
    "            self.bias = self.bias - self.lr*db\n",
    "            \n",
    "           \n",
    "    def predict(self, X):\n",
    "        print(self.weights)\n",
    "        linear_pred = np.dot(X, self.weights)+ self.bias\n",
    "        y_pred = sigmoid(linear_pred)\n",
    "        class_pred=[]\n",
    "        for i in y_pred:\n",
    "            if i>=0.5:\n",
    "                class_pred.append(1)\n",
    "            else:\n",
    "                class_pred.append(0)\n",
    "        return class_pred\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "ec8e3e42-2b05-4139-af38-890c5ce742a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "X_0=df.drop(columns=[\"y\"])\n",
    "y_0=df.drop(columns=[\"x_1\", \"x_2\"])\n",
    "X=X_0.to_numpy()\n",
    "y=y_0.to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "7f53f60f-318f-43c6-9859-00b8faf8e3a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "4b9bbd99-b172-4e0b-9cf7-6999b9277753",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.00338285 -0.05892104]\n",
      "0.82\n"
     ]
    }
   ],
   "source": [
    "# Testing data\n",
    "df1=pd.read_csv('data/ds1_test.csv')\n",
    "X_0=df1.drop(columns=[\"y\"])\n",
    "y_0=df1.drop(columns=[\"x_1\", \"x_2\"])\n",
    "X1=X_0.to_numpy()\n",
    "y1=y_0.to_numpy().flatten()\n",
    "\n",
    "y_pred=clf.predict(X1)\n",
    "\n",
    "\n",
    "def accuracy(y_pred, y_test):\n",
    "    return np.sum(y_pred==y_test)/len(y_test)\n",
    "\n",
    "acc = accuracy(y_pred, y1)\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
